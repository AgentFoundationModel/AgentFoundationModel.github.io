<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-02-06 Tue 18:27 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>An Interactive Agent Foundation Model</title>
<meta name="author" content="Bidipta Sarkar" />
<meta name="description" content="An Interactive Agent Foundation Model" />
<meta name="keywords" content="homepage, website, research, AI" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
<link rel="stylesheet" type="text/css" href="style.css"/>
<link rel="stylesheet" type="text/css" href="bootstrap.min.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
<script src="js/app.js"></script>
</head>
<body>
<div id="content" class="content">
<div id="main" class="container">


<p>
<div class="row"><h2 class="col-md-12 text-center"><strong><font size="+4r">  An Interactive Agent Foundation Model </font></strong></h2></div><br />
</p>


<div class="row"> <div class="col-md-12 text-center">
<ul class="org-ul list-inline">
<li>Zane Durante<br /></li>
<li>Bidipta Sarkar<br /></li>
<li>Ran Gong<br /></li>
<li>Rohan Taori<br /></li>
<li>Yusuke Noda<br /></li>
<li>Paul Tang<br /></li>
<li>Ehsan Adeli<br /></li>
<li>Shrinidhi Kowshika Lakshmikanth<br /></li>
<li>Kevin Schulman<br /></li>
<li>Arnold Milstein<br /></li>
<li>Demetri Terzopoulos<br /></li>
<li>Ade Famoti<br /></li>
<li>Noboru Kuno<br /></li>
<li>Ashley Llorens<br /></li>
<li>Hoi Vo<br /></li>
<li>Katsu Ikeuchi<br /></li>
<li>Li Fei-Fei<br /></li>
<li>Jianfeng Gao<br /></li>
<li>Naoki Wake<br /></li>
<li>Qiuyuan Huang<br /></li>
</ul>


</div> </div>


<div class="row"> <div class="col-md-12 text-center">
<ul class="org-ul list-inline">
<li><image src="img/msr_logo.gif" height="55px"><br /></li>
<li><image src="img/stanford_logo.png" height="48px"><br /></li>
<li><image src="img/ucla_logo.png" height="48px"><br /></li>
</ul>
</div> </div>

<div class="row"> <div class="col-md-4 col-md-offset-4 text-center">
<ul class="org-ul nav nav-pills nav-justified">
<li><a href="pdfs/paper.pdf"><image src="img/paper.png" height="60px"><h4><strong>Paper</strong></h4></a><br /></li>
<li><image src="img/huggingface.svg" height="60px"><h4><strong>Model (Coming Soon!)</strong></h4><br /></li>
</ul>

</div></div>

<div class="row"> <div class="col-md-8 col-md-offset-2">

<div id="outline-container-org75b9be5" class="outline-2">
<h2 id="org75b9be5"></h2>
<div class="outline-text-2" id="text-org75b9be5">
</div>
<div id="outline-container-orgb197740" class="outline-3">
<h3 id="orgb197740">Abstract</h3>
<div class="outline-text-3" id="text-orgb197740">
<p>
The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains &#x2013; Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems.<br />
</p>
</div>
</div>

<div id="outline-container-orgf2b50c3" class="outline-3">
<h3 id="orgf2b50c3">Robotics Examples</h3>
<div class="outline-text-3" id="text-orgf2b50c3">
<p>
The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains &#x2013; Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems.<br />
</p>
</div>
</div>



<div id="outline-container-org613b1b8" class="outline-3">
<h3 id="org613b1b8">Gaming Examples</h3>
<div class="outline-text-3" id="text-org613b1b8">
<p>
The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains &#x2013; Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems.<br />
</p>
</div>
</div>


<div id="outline-container-orgad0b88a" class="outline-3">
<h3 id="orgad0b88a">Healthcare Examples</h3>
<div class="outline-text-3" id="text-orgad0b88a">
<p>
The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains &#x2013; Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems.<br />
</p>



</div>








</div></div></div>

<div class="row">
  <div class="col-md-8 col-md-offset-2">
    <!-- <h3> -->
      <!--     Acknowledgements -->
      <!-- </h3> -->
    <p class="text-justify">
      <br><br>
      The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a> and <a href="https://robotics-transformer.github.io/">RT-1</a>
    </p>
  </div>
</div>
</div>
</div>
</div>
</div>
</body>
</html>